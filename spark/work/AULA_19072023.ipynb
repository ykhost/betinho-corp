{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84528e0a-289c-493f-a373-ac05e30a063f",
   "metadata": {},
   "source": [
    "## SPARK SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b3320a-2d2a-4eb0-85b8-2c00f964897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Streaming').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436f1045-a1dd-4acd-adb2-9fc323cdb667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Aula</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f93ab322530>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2add37ce-9411-4bcd-9c23-4d0a2b583ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData (qtde):\n",
    "    list = []\n",
    "    for x in range (qtde):\n",
    "        print(x)\n",
    "        r = requests.get('https://random-data-api.com/api/v2/users')\n",
    "        list.append(r.json())\n",
    "        req = spark.read.json(spark.sparkContext.parallelize(list))\n",
    "        req = req.select( \\\n",
    "         'email' \\\n",
    "        ,'first_name' \\\n",
    "        ,'last_name' \\\n",
    "        ,'gender' \\\n",
    "        ,'id' \\\n",
    "        ,'username' \\\n",
    "                 )\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18ead4c-a76e-4679-8984-7555de9d2284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "df = loadData(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f64c4a-d8a1-4681-b532-deeb3861d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.parquet('/datalake/raw/api',mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a32cb-ed97-4d2d-9feb-b05818048781",
   "metadata": {},
   "source": [
    "#### executar no terminal\n",
    " spark-submit app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a3c76-1074-4f86-a8de-ca2cac753946",
   "metadata": {},
   "source": [
    "## STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76538acb-3d2f-4fe6-8133-bfb57138e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Streaming').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a161715-eea7-4b6e-8770-51e95714a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers','kafka-broker:9092') \\\n",
    "    .option('subscribe','atividade') \\\n",
    "    .option('startingOffsets','earliest') \\\n",
    "    .option('kafka.group.id','spark3') \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f449a60e-4ee0-48ef-8abc-eaae601cc0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd39a16-5b2e-42fe-a5a0-bca8622ea001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6239be-ea4d-4154-a8a1-90668bec67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, from_json, col\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06e5fed-b6d0-4b65-969e-e9270fce808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"activity\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"participants\", IntegerType()),\n",
    "    StructField(\"price\", DoubleType()),\n",
    "    StructField(\"link\", StringType()),\n",
    "    StructField(\"key\", StringType()),\n",
    "    StructField(\"accessibility\", DoubleType())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af793691-0c2d-4295-8a3e-0342aafd6120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('activity', StringType(), True), StructField('type', StringType(), True), StructField('participants', IntegerType(), True), StructField('price', DoubleType(), True), StructField('link', StringType(), True), StructField('key', StringType(), True), StructField('accessibility', DoubleType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4c562ff-6aea-44fa-8520-b0f734558b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_df = df.select(from_json(col('value').cast('string'),schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e273a70-d718-472a-8caa-8b879db93fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff568df0-a599-478f-8a8c-5b06ce7422d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = value_df.selectExpr('value.activity','value.type','value.key','value.participants','value.price','value.link','value.accessibility')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d6891-bce8-46ed-9769-97db498657ef",
   "metadata": {},
   "source": [
    "### SALVAR NA MEMÃ“RIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47e487d3-6f8c-4836-a336-488ac55e4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = exploded.writeStream \\\n",
    "    .queryName('qraw') \\\n",
    "    .format('memory') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11cb92e8-1e14-4da7-b0d6-157065e870a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b278cd2e-c45e-4a45-9deb-da065e8ef263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from qraw').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e276cd5-a144-4493-8271-cb7ed3e0cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ceccd9-62ec-481e-8160-c2c360b39ca2",
   "metadata": {},
   "source": [
    "### SALVAR EM DISCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d07272ff-3921-4b57-a162-802034eb05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDS = exploded.writeStream \\\n",
    "    .format('json') \\\n",
    "    .option('path','/datalake/raw/streaming') \\\n",
    "    .option('checkpointLocation','/spark/chkpoint') \\\n",
    "    .outputMode('append') \\\n",
    "    .queryName('SS WRITE') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f3a2b-74eb-4335-91e9-2af714f9f0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
